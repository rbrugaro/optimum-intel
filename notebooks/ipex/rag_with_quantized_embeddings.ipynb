{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6195da33-34c3-4ca2-943a-050b6dcbacbc",
   "metadata": {},
   "source": [
    "# RAG pipeline using Optimized and Quantized Embedders\n",
    "\n",
    "In this tutorial, we will demo how to build a RAG pipeline, with the embedding for all documents done using Quantized Embedders.\n",
    "\n",
    "We will use a pipeline that will:\n",
    "\n",
    "* Create a document collection.\n",
    "* Embed all documents using Quantized Embedders.\n",
    "* Fetch relevant documents for our question.\n",
    "* Run an LLM answer the question.\n",
    "\n",
    "For more information about optimized models, we refer to [optimum-intel](https://github.com/huggingface/optimum-intel.git) and [IPEX](https://github.com/intel/intel-extension-for-pytorch).\n",
    "\n",
    "This tutorial is based on the [Langchain RAG tutorial here](https://towardsai.net/p/machine-learning/dense-x-retrieval-technique-in-langchain-and-llamaindex) and [Langchain cookbook here] (https://github.com/langchain-ai/langchain/blob/master/cookbook/rag_with_quantized_embeddings.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26db2da5-3733-4a90-909e-6c11508ea140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from pathlib import Path\n",
    "\n",
    "import langchain\n",
    "import torch\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryByteStore, LocalFileStore\n",
    "from langchain_community.document_loaders.recursive_url_loader import (\n",
    "    RecursiveUrlLoader,\n",
    ")\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# For our example, we'll load docs from the web\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "DOCSTORE_DIR = \".\"\n",
    "DOCSTORE_ID_KEY = \"doc_id\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ccda4e-7af5-4355-b9c4-25547edf33f9",
   "metadata": {},
   "source": [
    "Lets first load up this paper, and split into text chunks of size 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f4d8888-53a6-49f5-a198-da5c92419ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 documents\n",
      "Split into 53 documents\n"
     ]
    }
   ],
   "source": [
    "# Could add more parsing here, as it's very raw.\n",
    "loader = RecursiveUrlLoader(\n",
    "    \"https://ar5iv.labs.arxiv.org/html/1706.03762\",\n",
    "    max_depth=2,\n",
    "    extractor=lambda x: Soup(x, \"html.parser\").text,\n",
    ")\n",
    "data = loader.load()\n",
    "print(f\"Loaded {len(data)} documents\")\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "print(f\"Split into {len(all_splits)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e90632-2ac2-49eb-80da-ffe9ac4a278d",
   "metadata": {},
   "source": [
    "In order to embed our documents, we can use the ```QuantizedBiEncoderEmbeddings```, for efficient and fast embedding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a68a6f6-332d-481e-bbea-ad763155ea36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rbrugaro/miniforge3/envs/llama-index/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Passing the argument `library_name` to `get_supported_tasks_for_model_type` is required, but got library_name=None. Defaulting to `transformers`. An error will be raised in a future version of Optimum if `library_name` is not provided.\n",
      "/home/rbrugaro/miniforge3/envs/llama-index/lib/python3.10/site-packages/torch/amp/autocast_mode.py:267: UserWarning: In CPU autocast, but the target dtype is not supported. Disabling autocast.\n",
      "CPU Autocast only supports dtype of torch.bfloat16, torch.float16 currently.\n",
      "  warnings.warn(error_message)\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import QuantizedBiEncoderEmbeddings\n",
    "from langchain_core.embeddings import Embeddings\n",
    "\n",
    "model_name = \"Intel/bge-small-en-v1.5-rag-int8-static\"\n",
    "encode_kwargs = {\"normalize_embeddings\": True}  # set True to compute cosine similarity\n",
    "\n",
    "model_inc = QuantizedBiEncoderEmbeddings(\n",
    "    model_name=model_name,\n",
    "    encode_kwargs=encode_kwargs,\n",
    "    query_instruction=\"Represent this sentence for searching relevant passages: \",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360b2837-8024-47e0-a4ba-592505a9a5c8",
   "metadata": {},
   "source": [
    "With our embedder in place, lets define our retriever:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "18bc0a73-1a13-4b2f-96ac-05a5313343b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multi_vector_retriever(\n",
    "    docstore_id_key: str, collection_name: str, embedding_function: Embeddings\n",
    "):\n",
    "    \"\"\"Create the composed retriever object.\"\"\"\n",
    "    vectorstore = Chroma(\n",
    "        collection_name=collection_name,\n",
    "        embedding_function=embedding_function,\n",
    "    )\n",
    "    store = InMemoryByteStore()\n",
    "\n",
    "    return MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        byte_store=store,\n",
    "        id_key=docstore_id_key,\n",
    "    )\n",
    "\n",
    "\n",
    "retriever = get_multi_vector_retriever(DOCSTORE_ID_KEY, \"multi_vec_store\", model_inc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8484078e-1bf0-4080-a354-ef23823fd6dc",
   "metadata": {},
   "source": [
    "Next, we divide each chunk into sub-docs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e12f48d4-6562-416b-8f28-342912e5756e",
   "metadata": {},
   "outputs": [],
   "source": [
    "child_text_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "id_key = \"doc_id\"\n",
    "doc_ids = [str(uuid.uuid4()) for _ in all_splits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a268ef5f-91c2-4d8e-87f0-53db376e6a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_docs = []\n",
    "for i, doc in enumerate(all_splits):\n",
    "    _id = doc_ids[i]\n",
    "    _sub_docs = child_text_splitter.split_documents([doc])\n",
    "    for _doc in _sub_docs:\n",
    "        _doc.metadata[id_key] = _id\n",
    "    sub_docs.extend(_sub_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84ea8f4-a5de-4d76-b44d-85e56583f489",
   "metadata": {},
   "source": [
    "Lets write our documents into our new store. This will use our embedder on each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1af831ce-0eae-44bc-aca7-4d691063640b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rbrugaro/miniforge3/envs/llama-index/lib/python3.10/site-packages/torch/amp/autocast_mode.py:267: UserWarning: In CPU autocast, but the target dtype is not supported. Disabling autocast.\n",
      "CPU Autocast only supports dtype of torch.bfloat16, torch.float16 currently.\n",
      "  warnings.warn(error_message)\n",
      "Batches: 100%|██████████| 6/6 [00:01<00:00,  3.46it/s]\n"
     ]
    }
   ],
   "source": [
    "retriever.vectorstore.add_documents(sub_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, all_splits)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580bc212-8ecd-4d28-8656-b96fcd0d7eb6",
   "metadata": {},
   "source": [
    "Great! Our retriever is good to go. Lets load up an LLM, that will reason over the retrieved documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "008c992f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rbrugaro/miniforge3/envs/llama-index/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Detect torchscript is false. Convert to torchscript model!\n",
      "Framework not specified. Using pt to export the model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "264eca4240f24daab2d5998f6dc48b6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing the argument `library_name` to `get_supported_tasks_for_model_type` is required, but got library_name=None. Defaulting to `transformers`. An error will be raised in a future version of Optimum if `library_name` is not provided.\n",
      "/home/rbrugaro/miniforge3/envs/llama-index/lib/python3.10/site-packages/transformers/modeling_utils.py:4481: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "/home/rbrugaro/miniforge3/envs/llama-index/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:276: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  elif sliding_window is None or key_value_length < sliding_window:\n",
      "/home/rbrugaro/miniforge3/envs/llama-index/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:114: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if (input_shape[-1] > 1 or self.sliding_window is not None) and self.is_causal:\n",
      "/home/rbrugaro/miniforge3/envs/llama-index/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:162: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if past_key_values_length > 0:\n",
      "/home/rbrugaro/miniforge3/envs/llama-index/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:119: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if seq_len > self.max_seq_len_cached:\n",
      "/home/rbrugaro/miniforge3/envs/llama-index/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:662: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n",
      "Passing the argument `library_name` to `get_supported_tasks_for_model_type` is required, but got library_name=None. Defaulting to `transformers`. An error will be raised in a future version of Optimum if `library_name` is not provided.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from optimum.intel.ipex import IPEXModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "model_id = \"Intel/neural-chat-7b-v3-3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = IPEXModelForCausalLM.from_pretrained(\n",
    "    model_id, torch_dtype=torch.bfloat16, export=True\n",
    ")\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=300, min_new_tokens=300)\n",
    "\n",
    "hf = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd21fb2-0442-477d-aae2-9e7ee1d1d778",
   "metadata": {},
   "source": [
    "Next, we will load up a prompt for answering questions using retrieved documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5e582509-caaf-4920-932c-4ce16162c789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "60cba289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))]\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdfcba5-7ec7-4d0a-820e-4e200643a882",
   "metadata": {},
   "source": [
    "We can now build our pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b74d8dfb-72bb-46da-9df9-0dc47a3ac791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "rag_chain = {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | hf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc53602-86d6-420f-91b1-fc2effa7e986",
   "metadata": {},
   "source": [
    "Excellent! lets ask it a question.\n",
    "We will also use a verbose and debug, to check which documents were used by the model to produce the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f0a92c07-53da-4e1f-b880-ee83a36ee17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is the first transduction model relying entirely on self-attention?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is the first transduction model relying entirely on self-attention?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is the first transduction model relying entirely on self-attention?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnablePassthrough] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What is the first transduction model relying entirely on self-attention?\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rbrugaro/miniforge3/envs/llama-index/lib/python3.10/site-packages/torch/amp/autocast_mode.py:267: UserWarning: In CPU autocast, but the target dtype is not supported. Disabling autocast.\n",
      "CPU Autocast only supports dtype of torch.bfloat16, torch.float16 currently.\n",
      "  warnings.warn(error_message)\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "access to the `model_dtype` attribute is deprecated and will be removed after v1.18.0, please use `_dtype` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question>] [281ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [2ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:HuggingFacePipeline] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: What is the first transduction model relying entirely on self-attention? \\nContext: [Document(metadata={'source': 'https://ar5iv.labs.arxiv.org/html/1706.03762', 'content_type': 'text/html; charset=utf-8', 'title': '[1706.03762] Attention Is All You Need', 'language': 'en'}, page_content='Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\\\n\\\\n\\\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34].\\\\n\\\\n\\\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution.\\\\nIn the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9].'), Document(metadata={'source': 'https://ar5iv.labs.arxiv.org/html/1706.03762', 'content_type': 'text/html; charset=utf-8', 'title': '[1706.03762] Attention Is All You Need', 'language': 'en'}, page_content='Our results in Table\\\\xa04 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8].\\\\n\\\\n\\\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the BerkeleyParser [29] even when training only on the WSJ training set of 40K sentences.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n7 Conclusion\\\\n\\\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.')] \\nAnswer:\"\n",
      "  ]\n",
      "}\n",
      "pipeline kwards {}\n",
      "<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f8f4d07aa40>\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:HuggingFacePipeline] [45.59s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Human: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: What is the first transduction model relying entirely on self-attention? \\nContext: [Document(metadata={'source': 'https://ar5iv.labs.arxiv.org/html/1706.03762', 'content_type': 'text/html; charset=utf-8', 'title': '[1706.03762] Attention Is All You Need', 'language': 'en'}, page_content='Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\\\n\\\\n\\\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34].\\\\n\\\\n\\\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution.\\\\nIn the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9].'), Document(metadata={'source': 'https://ar5iv.labs.arxiv.org/html/1706.03762', 'content_type': 'text/html; charset=utf-8', 'title': '[1706.03762] Attention Is All You Need', 'language': 'en'}, page_content='Our results in Table\\\\xa04 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8].\\\\n\\\\n\\\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the BerkeleyParser [29] even when training only on the WSJ training set of 40K sentences.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n7 Conclusion\\\\n\\\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.')] \\nAnswer: The first transduction model relying entirely on self-attention is the Transformer. It was introduced in the paper \\\"Attention Is All You Need.\\\" This model computes representations of its input and output without using sequence-aligned RNNs or convolution. It replaces recurrent layers in encoder-decoder architectures with multi-headed self-attention. The Transformer outperforms RNN sequence-to-sequence models and other attention-based models. It was designed for sequence transduction tasks like machine translation, text summarization, and question answering. The Transformer architecture is also known as the Encoder-Decoder with Self-Attention. It has been successfully applied in various natural language processing tasks. The Transformer architecture is also known as the Encoder-Decoder with Self-Attention. It has been successfully applied in various natural language processing tasks. The Transformer architecture is also known as the Encoder-Decoder with Self-Attention. It has been successfully applied in various natural language processing tasks. The Transformer architecture is also known as the Encoder-Decoder with Self-Attention. It has been successfully applied in various natural language processing tasks. The Transformer architecture is also known as the Encoder-Decoder with Self-Attention. It has been successfully applied in various natural language processing tasks. The Transformer architecture is also known as the Encoder-Decoder with Self-Attention. It has been successfully applied in\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [45.88s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"Human: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: What is the first transduction model relying entirely on self-attention? \\nContext: [Document(metadata={'source': 'https://ar5iv.labs.arxiv.org/html/1706.03762', 'content_type': 'text/html; charset=utf-8', 'title': '[1706.03762] Attention Is All You Need', 'language': 'en'}, page_content='Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\\\n\\\\n\\\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34].\\\\n\\\\n\\\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution.\\\\nIn the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9].'), Document(metadata={'source': 'https://ar5iv.labs.arxiv.org/html/1706.03762', 'content_type': 'text/html; charset=utf-8', 'title': '[1706.03762] Attention Is All You Need', 'language': 'en'}, page_content='Our results in Table\\\\xa04 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8].\\\\n\\\\n\\\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the BerkeleyParser [29] even when training only on the WSJ training set of 40K sentences.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n7 Conclusion\\\\n\\\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.')] \\nAnswer: The first transduction model relying entirely on self-attention is the Transformer. It was introduced in the paper \\\"Attention Is All You Need.\\\" This model computes representations of its input and output without using sequence-aligned RNNs or convolution. It replaces recurrent layers in encoder-decoder architectures with multi-headed self-attention. The Transformer outperforms RNN sequence-to-sequence models and other attention-based models. It was designed for sequence transduction tasks like machine translation, text summarization, and question answering. The Transformer architecture is also known as the Encoder-Decoder with Self-Attention. It has been successfully applied in various natural language processing tasks. The Transformer architecture is also known as the Encoder-Decoder with Self-Attention. It has been successfully applied in various natural language processing tasks. The Transformer architecture is also known as the Encoder-Decoder with Self-Attention. It has been successfully applied in various natural language processing tasks. The Transformer architecture is also known as the Encoder-Decoder with Self-Attention. It has been successfully applied in various natural language processing tasks. The Transformer architecture is also known as the Encoder-Decoder with Self-Attention. It has been successfully applied in various natural language processing tasks. The Transformer architecture is also known as the Encoder-Decoder with Self-Attention. It has been successfully applied in\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "langchain.verbose = True\n",
    "langchain.debug = True\n",
    "\n",
    "llm_res = rag_chain.invoke(\n",
    "    \"What is the first transduction model relying entirely on self-attention?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a410cec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is the first transduction model relying entirely on self-attention?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is the first transduction model relying entirely on self-attention?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is the first transduction model relying entirely on self-attention?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnablePassthrough] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What is the first transduction model relying entirely on self-attention?\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rbrugaro/miniforge3/envs/llama-index/lib/python3.10/site-packages/torch/amp/autocast_mode.py:267: UserWarning: In CPU autocast, but the target dtype is not supported. Disabling autocast.\n",
      "CPU Autocast only supports dtype of torch.bfloat16, torch.float16 currently.\n",
      "  warnings.warn(error_message)\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "access to the `model_dtype` attribute is deprecated and will be removed after v1.18.0, please use `_dtype` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question>] [61ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [2ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:HuggingFacePipeline] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: What is the first transduction model relying entirely on self-attention? \\nContext: [Document(metadata={'source': 'https://ar5iv.labs.arxiv.org/html/1706.03762', 'content_type': 'text/html; charset=utf-8', 'title': '[1706.03762] Attention Is All You Need', 'language': 'en'}, page_content='Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\\\n\\\\n\\\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34].\\\\n\\\\n\\\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution.\\\\nIn the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9].'), Document(metadata={'source': 'https://ar5iv.labs.arxiv.org/html/1706.03762', 'content_type': 'text/html; charset=utf-8', 'title': '[1706.03762] Attention Is All You Need', 'language': 'en'}, page_content='Our results in Table\\\\xa04 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8].\\\\n\\\\n\\\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the BerkeleyParser [29] even when training only on the WSJ training set of 40K sentences.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n7 Conclusion\\\\n\\\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.')] \\nAnswer:\"\n",
      "  ]\n",
      "}\n",
      "pipeline kwards {}\n",
      "<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f8f4d07aa40>\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:HuggingFacePipeline] [44.53s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Human: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: What is the first transduction model relying entirely on self-attention? \\nContext: [Document(metadata={'source': 'https://ar5iv.labs.arxiv.org/html/1706.03762', 'content_type': 'text/html; charset=utf-8', 'title': '[1706.03762] Attention Is All You Need', 'language': 'en'}, page_content='Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\\\n\\\\n\\\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34].\\\\n\\\\n\\\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution.\\\\nIn the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9].'), Document(metadata={'source': 'https://ar5iv.labs.arxiv.org/html/1706.03762', 'content_type': 'text/html; charset=utf-8', 'title': '[1706.03762] Attention Is All You Need', 'language': 'en'}, page_content='Our results in Table\\\\xa04 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8].\\\\n\\\\n\\\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the BerkeleyParser [29] even when training only on the WSJ training set of 40K sentences.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n7 Conclusion\\\\n\\\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.')] \\nAnswer: The first transduction model relying entirely on self-attention is the Transformer. It was introduced in the paper \\\"Attention Is All You Need.\\\" This model computes representations of its input and output without using sequence-aligned RNNs or convolution. It replaces recurrent layers in encoder-decoder architectures with multi-headed self-attention. The Transformer outperforms RNN sequence-to-sequence models and other attention-based models. It was designed for sequence transduction tasks like machine translation, text summarization, and question answering. The Transformer architecture is also known as the Encoder-Decoder with Self-Attention. It has been successfully applied in various natural language processing tasks. The Transformer architecture is also known as the Encoder-Decoder with Self-Attention. It has been successfully applied in various natural language processing tasks. The Transformer architecture is also known as the Encoder-Decoder with Self-Attention. It has been successfully applied in various natural language processing tasks. The Transformer architecture is also known as the Encoder-Decoder with Self-Attention. It has been successfully applied in various natural language processing tasks. The Transformer architecture is also known as the Encoder-Decoder with Self-Attention. It has been successfully applied in various natural language processing tasks. The Transformer architecture is also known as the Encoder-Decoder with Self-Attention. It has been successfully applied in\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [44.60s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"Human: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: What is the first transduction model relying entirely on self-attention? \\nContext: [Document(metadata={'source': 'https://ar5iv.labs.arxiv.org/html/1706.03762', 'content_type': 'text/html; charset=utf-8', 'title': '[1706.03762] Attention Is All You Need', 'language': 'en'}, page_content='Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\\\n\\\\n\\\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34].\\\\n\\\\n\\\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution.\\\\nIn the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9].'), Document(metadata={'source': 'https://ar5iv.labs.arxiv.org/html/1706.03762', 'content_type': 'text/html; charset=utf-8', 'title': '[1706.03762] Attention Is All You Need', 'language': 'en'}, page_content='Our results in Table\\\\xa04 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8].\\\\n\\\\n\\\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the BerkeleyParser [29] even when training only on the WSJ training set of 40K sentences.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n7 Conclusion\\\\n\\\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.')] \\nAnswer: The first transduction model relying entirely on self-attention is the Transformer. It was introduced in the paper \\\"Attention Is All You Need.\\\" This model computes representations of its input and output without using sequence-aligned RNNs or convolution. It replaces recurrent layers in encoder-decoder architectures with multi-headed self-attention. The Transformer outperforms RNN sequence-to-sequence models and other attention-based models. It was designed for sequence transduction tasks like machine translation, text summarization, and question answering. The Transformer architecture is also known as the Encoder-Decoder with Self-Attention. It has been successfully applied in various natural language processing tasks. The Transformer architecture is also known as the Encoder-Decoder with Self-Attention. It has been successfully applied in various natural language processing tasks. The Transformer architecture is also known as the Encoder-Decoder with Self-Attention. It has been successfully applied in various natural language processing tasks. The Transformer architecture is also known as the Encoder-Decoder with Self-Attention. It has been successfully applied in various natural language processing tasks. The Transformer architecture is also known as the Encoder-Decoder with Self-Attention. It has been successfully applied in various natural language processing tasks. The Transformer architecture is also known as the Encoder-Decoder with Self-Attention. It has been successfully applied in\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "langchain.verbose = True\n",
    "langchain.debug = True\n",
    "\n",
    "llm_res = rag_chain.invoke(\n",
    "    \"What is the first transduction model relying entirely on self-attention?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaefd01-254a-445d-a95f-37889c126e0e",
   "metadata": {},
   "source": [
    "Based on the retrieved documents, the answer is indeed correct :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
